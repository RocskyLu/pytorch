# WARNING: DO NOT EDIT THIS FILE DIRECTLY!!!
# See the README.md in this directory.

# IMPORTANT: To update Docker image version, please follow
# the instructions at
# https://github.com/pytorch/pytorch/wiki/Docker-image-build-on-CircleCI

version: 2.1

parameters:
  run_binary_tests:
    type: boolean
    default: false
  run_build:
    type: boolean
    default: true

# docker_config_defaults: &docker_config_defaults
#   user: jenkins
#   aws_auth:
#     # This IAM user only allows read-write access to ECR
#     aws_access_key_id: ${CIRCLECI_AWS_ACCESS_KEY_FOR_ECR_READ_WRITE_V4}
#     aws_secret_access_key: ${CIRCLECI_AWS_SECRET_KEY_FOR_ECR_READ_WRITE_V4}

executors:
  windows-with-nvidia-gpu:
    machine:
      resource_class: windows.gpu.nvidia.medium
      image: windows-server-2019-nvidia:stable
      shell: bash.exe

  windows-xlarge-cpu-with-nvidia-cuda:
    machine:
      resource_class: windows.xlarge
      image: windows-server-2019-vs2019:stable
      shell: bash.exe

  windows-medium-cpu-with-nvidia-cuda:
    machine:
      resource_class: windows.medium
      image: windows-server-2019-vs2019:stable
      shell: bash.exe
commands:

  # calculate_docker_image_tag:
  #   description: "Calculates the docker image tag"
  #   steps:
  #     - run:
  #         name: "Calculate docker image hash"
  #         command: |
  #           DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
  #           echo "DOCKER_TAG=${DOCKER_TAG}" >> "${BASH_ENV}"

  designate_upload_channel:
    description: "inserts the correct upload channel into ${BASH_ENV}"
    steps:
      - run:
          name: adding UPLOAD_CHANNEL to BASH_ENV
          command: |
            our_upload_channel=nightly
            # On tags upload to test instead
            if [[ -n "${CIRCLE_TAG}" ]]; then
              our_upload_channel=test
            fi
            echo "export UPLOAD_CHANNEL=${our_upload_channel}" >> ${BASH_ENV}

  # This system setup script is meant to run before the CI-related scripts, e.g.,
  # installing Git client, checking out code, setting up CI env, and
  # building/testing.
  setup_linux_system_environment:
    steps:
      - run:
          name: Set Up System Environment
          no_output_timeout: "1h"
          command: .circleci/scripts/setup_linux_system_environment.sh

  setup_ci_environment:
    steps:
      - run:
          name: Set Up CI Environment After attach_workspace
          no_output_timeout: "1h"
          command: .circleci/scripts/setup_ci_environment.sh

  # brew_update:
  #   description: "Update Homebrew and install base formulae"
  #   steps:
  #     - run:
  #         name: Update Homebrew
  #         no_output_timeout: "10m"
  #         command: |
  #           set -ex

  #           # Update repositories manually.
  #           # Running `brew update` produces a comparison between the
  #           # current checkout and the updated checkout, which takes a
  #           # very long time because the existing checkout is 2y old.
  #           for path in $(find /usr/local/Homebrew -type d -name .git)
  #           do
  #           cd $path/..
  #           git fetch --depth=1 origin
  #           git reset --hard origin/master
  #           done

  #           export HOMEBREW_NO_AUTO_UPDATE=1

  #           # Install expect and moreutils so that we can call `unbuffer` and `ts`.
  #           # moreutils installs a `parallel` executable by default, which conflicts
  #           # with the executable from the GNU `parallel`, so we must unlink GNU
  #           # `parallel` first, and relink it afterwards.
  #           brew unlink parallel
  #           brew install moreutils
  #           brew link parallel --overwrite
  #           brew install expect

  # brew_install:
  #   description: "Install Homebrew formulae"
  #   parameters:
  #     formulae:
  #       type: string
  #       default: ""
  #   steps:
  #     - run:
  #         name: Install << parameters.formulae >>
  #         no_output_timeout: "10m"
  #         command: |
  #           set -ex
  #           export HOMEBREW_NO_AUTO_UPDATE=1
  #           brew install << parameters.formulae >>

  # run_brew_for_macos_build:
  #   steps:
  #     - brew_update
  #     - brew_install:
  #         formulae: libomp

  # run_brew_for_ios_build:
  #   steps:
  #     - brew_update
  #     - brew_install:
  #         formulae: libtool

  optional_merge_target_branch:
    steps:
      - run:
          name: (Optional) Merge target branch
          no_output_timeout: "10m"
          command: |
            if [[ -n "$CIRCLE_PULL_REQUEST" && "$CIRCLE_BRANCH" != "nightly" ]]; then
              PR_NUM=$(basename $CIRCLE_PULL_REQUEST)
              CIRCLE_PR_BASE_BRANCH=$(curl -s https://api.github.com/repos/$CIRCLE_PROJECT_USERNAME/$CIRCLE_PROJECT_REPONAME/pulls/$PR_NUM | jq -r '.base.ref')
              if [[ "${BUILD_ENVIRONMENT}" == *"xla"* || "${BUILD_ENVIRONMENT}" == *"gcc5"* ]] ; then
                set -x
                git config --global user.email "circleci.ossci@gmail.com"
                git config --global user.name "CircleCI"
                git config remote.origin.url https://github.com/pytorch/pytorch.git
                git config --add remote.origin.fetch +refs/heads/release/1.8:refs/remotes/origin/release/1.8
                git fetch --tags --progress https://github.com/pytorch/pytorch.git +refs/heads/release/1.8:refs/remotes/origin/release/1.8 --depth=100 --quiet
                # PRs generated from ghstack has format CIRCLE_PR_BASE_BRANCH=gh/xxx/1234/base
                if [[ "${CIRCLE_PR_BASE_BRANCH}" == "gh/"* ]]; then
                  CIRCLE_PR_BASE_BRANCH=release/1.8
                fi
                export GIT_MERGE_TARGET=`git log -n 1 --pretty=format:"%H" origin/$CIRCLE_PR_BASE_BRANCH`
                echo "GIT_MERGE_TARGET: " ${GIT_MERGE_TARGET}
                export GIT_COMMIT=${CIRCLE_SHA1}
                echo "GIT_COMMIT: " ${GIT_COMMIT}
                git checkout -f ${GIT_COMMIT}
                git reset --hard ${GIT_COMMIT}
                git merge --allow-unrelated-histories --no-edit --no-ff ${GIT_MERGE_TARGET}
                echo "Merged $CIRCLE_PR_BASE_BRANCH branch before building in environment $BUILD_ENVIRONMENT"
                set +x
              else
                echo "No need to merge with $CIRCLE_PR_BASE_BRANCH, skipping..."
              fi
            else
              echo "This is not a pull request, skipping..."
            fi

  # upload_binary_size_for_android_build:
  #   description: "Upload binary size data for Android build"
  #   parameters:
  #     build_type:
  #       type: string
  #       default: ""
  #     artifacts:
  #       type: string
  #       default: ""
  #   steps:
  #     - run:
  #         name: "Binary Size - Install Dependencies"
  #         no_output_timeout: "5m"
  #         command: |
  #           retry () {
  #             $* || (sleep 1 && $*) || (sleep 2 && $*) || (sleep 4 && $*) || (sleep 8 && $*)
  #           }
  #           retry pip3 install requests
  #     - run:
  #         name: "Binary Size - Untar Artifacts"
  #         no_output_timeout: "5m"
  #         command: |
  #           # The artifact file is created inside docker container, which contains the result binaries.
  #           # Now unpackage it into the project folder. The subsequent script will scan project folder
  #           # to locate result binaries and report their sizes.
  #           # If artifact file is not provided it assumes that the project folder has been mounted in
  #           # the docker during build and already contains the result binaries, so this step can be skipped.
  #           export ARTIFACTS="<< parameters.artifacts >>"
  #           if [ -n "${ARTIFACTS}" ]; then
  #             tar xf "${ARTIFACTS}" -C ~/project
  #           fi
  #     - run:
  #         name: "Binary Size - Upload << parameters.build_type >>"
  #         no_output_timeout: "5m"
  #         command: |
  #           cd ~/project
  #           export ANDROID_BUILD_TYPE="<< parameters.build_type >>"
  #           export COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
  #           python3 .circleci/scripts/upload_binary_size_to_scuba.py android

##############################################################################
# Binary build (nightlies nightly build) defaults
# The binary builds use the docker executor b/c at time of writing the machine
# executor is limited to only two cores and is painfully slow (4.5+ hours per
# GPU build). But the docker executor cannot be run with --runtime=nvidia, and
# so the binary test/upload jobs must run on a machine executor. The package
# built in the build job is persisted to the workspace, which the test jobs
# expect. The test jobs just run a few quick smoke tests (very similar to the
# second-round-user-facing smoke tests above) and then upload the binaries to
# their final locations. The upload part requires credentials that should only
# be available to org-members.
#
# binary_checkout MUST be run before other commands here. This is because the
# other commands are written in .circleci/scripts/*.sh , so the pytorch source
# code must be downloaded on the machine before they can be run. We cannot
# inline all the code into this file, since that would cause the yaml size to
# explode past 4 MB (all the code in the command section is just copy-pasted to
# everywhere in the .circleci/config.yml file where it appears).
##############################################################################

# Checks out the Pytorch and Builder repos (always both of them), and places
# them in the right place depending on what executor we're running on. We curl
# our .sh file from the interweb to avoid yaml size bloat. Note that many jobs
# do not need both the pytorch and builder repos, so this is a little wasteful
# (smoke tests and upload jobs do not need the pytorch repo).
binary_checkout: &binary_checkout
  name: Checkout pytorch/builder repo
  command: .circleci/scripts/binary_checkout.sh

# Parses circleci arguments in a consistent way, essentially routing to the
# correct pythonXgccXcudaXos build we want
binary_populate_env: &binary_populate_env
  name: Set up binary env variables
  command: .circleci/scripts/binary_populate_env.sh

binary_install_miniconda: &binary_install_miniconda
  name: Install miniconda
  no_output_timeout: "1h"
  command: .circleci/scripts/binary_install_miniconda.sh

# This section is used in the binary_test and smoke_test jobs. It expects
# 'binary_populate_env' to have populated /home/circleci/project/env and it
# expects another section to populate /home/circleci/project/ci_test_script.sh
# with the code to run in the docker
binary_run_in_docker: &binary_run_in_docker
  name: Run in docker
  # This step only runs on circleci linux machine executors that themselves
  # need to start docker images
  command: .circleci/scripts/binary_run_in_docker.sh
##############################################################################
# Build parameters
##############################################################################
pytorch_params: &pytorch_params
  parameters:
    build_environment:
      type: string
      default: ""
    docker_image:
      type: string
      default: ""
    resource_class:
      type: string
      default: "large"
    use_cuda_docker_runtime:
      type: string
      default: ""
    build_only:
      type: string
      default: ""
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    DOCKER_IMAGE: << parameters.docker_image >>
    USE_CUDA_DOCKER_RUNTIME: << parameters.use_cuda_docker_runtime >>
    BUILD_ONLY: << parameters.build_only >>
  resource_class: << parameters.resource_class >>

# pytorch_ios_params: &pytorch_ios_params
#   parameters:
#     build_environment:
#       type: string
#       default: ""
#     ios_arch:
#       type: string
#       default: ""
#     ios_platform:
#       type: string
#       default: ""
#     op_list:
#       type: string
#       default: ""
#     use_metal:
#       type: string
#       default: "0"
#   environment:
#     BUILD_ENVIRONMENT: << parameters.build_environment >>
#     IOS_ARCH: << parameters.ios_arch >>
#     IOS_PLATFORM: << parameters.ios_platform >>
#     SELECTED_OP_LIST: << parameters.op_list >>
#     USE_PYTORCH_METAL: << parameters.use_metal >>

pytorch_windows_params: &pytorch_windows_params
  parameters:
    executor:
      type: string
      default: "windows-xlarge-cpu-with-nvidia-cuda"
    build_environment:
      type: string
      default: ""
    test_name:
      type: string
      default: ""
    cuda_version:
      type: string
      default: "10.1"
    python_version:
      type: string
      default: "3.6"
    vc_version:
      type: string
      default: "14.16"
    vc_year:
      type: string
      default: "2019"
    vc_product:
      type: string
      default: "BuildTools"
    use_cuda:
      type: string
      default: ""
  environment:
    BUILD_ENVIRONMENT: <<parameters.build_environment>>
    SCCACHE_BUCKET: "ossci-compiler-cache"
    CUDA_VERSION: <<parameters.cuda_version>>
    PYTHON_VERSION: <<parameters.python_version>>
    VC_VERSION: <<parameters.vc_version>>
    VC_YEAR: <<parameters.vc_year>>
    VC_PRODUCT: <<parameters.vc_product>>
    USE_CUDA: <<parameters.use_cuda>>
    TORCH_CUDA_ARCH_LIST: "7.5"
    JOB_BASE_NAME: <<parameters.test_name>>
    JOB_EXECUTOR: <<parameters.executor>>
# binary_linux_build_params: &binary_linux_build_params
#   parameters:
#     build_environment:
#       type: string
#       default: ""
#     docker_image:
#       type: string
#       default: ""
#     libtorch_variant:
#       type: string
#       default: ""
#     resource_class:
#       type: string
#       default: "2xlarge+"
#   environment:
#     BUILD_ENVIRONMENT: << parameters.build_environment >>
#     LIBTORCH_VARIANT: << parameters.libtorch_variant >>
#     ANACONDA_USER: pytorch
#   resource_class: << parameters.resource_class >>
#   docker:
#     - image: << parameters.docker_image >>

# binary_linux_test_upload_params: &binary_linux_test_upload_params
#   parameters:
#     build_environment:
#       type: string
#       default: ""
#     docker_image:
#       type: string
#       default: ""
#     libtorch_variant:
#       type: string
#       default: ""
#     resource_class:
#       type: string
#       default: "medium"
#     use_cuda_docker_runtime:
#       type: string
#       default: ""
#   environment:
#     BUILD_ENVIRONMENT: << parameters.build_environment >>
#     DOCKER_IMAGE: << parameters.docker_image >>
#     USE_CUDA_DOCKER_RUNTIME: << parameters.use_cuda_docker_runtime >>
#     LIBTORCH_VARIANT: << parameters.libtorch_variant >>
#   resource_class: << parameters.resource_class >>

# binary_mac_params: &binary_mac_params
#   parameters:
#     build_environment:
#       type: string
#       default: ""
#   environment:
#     BUILD_ENVIRONMENT: << parameters.build_environment >>

binary_windows_params: &binary_windows_params
  parameters:
    build_environment:
      type: string
      default: ""
    executor:
      type: string
      default: "windows-xlarge-cpu-with-nvidia-cuda"
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    BUILD_FOR_SYSTEM: windows
    JOB_EXECUTOR: <<parameters.executor>>
##############################################################################
# Job specs
##############################################################################
jobs:
  # pytorch_linux_build:
  #   <<: *pytorch_params
  #   machine:
  #     image: ubuntu-1604:202007-01
  #   steps:
  #   # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
  #   - checkout
  #   - calculate_docker_image_tag
  #   - setup_linux_system_environment
  #   - optional_merge_target_branch
  #   - setup_ci_environment
  #   - run:
  #       name: Build
  #       no_output_timeout: "1h"
  #       command: |
  #         set -e
  #         if [[ "${DOCKER_IMAGE}" == *rocm3.9* ]]; then
  #           export DOCKER_TAG="f3d89a32912f62815e4feaeed47e564e887dffd6"
  #         fi
  #         if [[ ${BUILD_ENVIRONMENT} == *"pure_torch"* ]]; then
  #           echo 'BUILD_CAFFE2=OFF' >> "${BASH_ENV}"
  #         fi
  #         if [[ ${BUILD_ENVIRONMENT} == *"paralleltbb"* ]]; then
  #           echo 'ATEN_THREADING=TBB' >> "${BASH_ENV}"
  #           echo 'USE_TBB=1' >> "${BASH_ENV}"
  #         elif [[ ${BUILD_ENVIRONMENT} == *"parallelnative"* ]]; then
  #           echo 'ATEN_THREADING=NATIVE' >> "${BASH_ENV}"
  #         fi
  #         echo "Parallel backend flags: "${PARALLEL_FLAGS}
  #         # Pull Docker image and run build
  #         echo "DOCKER_IMAGE: "${DOCKER_IMAGE}:${DOCKER_TAG}
  #         time docker pull ${DOCKER_IMAGE}:${DOCKER_TAG} >/dev/null
  #         export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${DOCKER_IMAGE}:${DOCKER_TAG})

  #         git submodule sync && git submodule update -q --init --recursive

  #         docker cp /home/circleci/project/. $id:/var/lib/jenkins/workspace

  #         export COMMAND='((echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/build.sh && find ${BUILD_ROOT} -type f -name "*.a" -or -name "*.o" -delete") | docker exec -u jenkins -i "$id" bash) 2>&1'

  #         echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  #         # Copy dist folder back
  #         docker cp $id:/var/lib/jenkins/workspace/dist /home/circleci/project/. || echo "Dist folder not found"

  #         # Push intermediate Docker image for next phase to use
  #         if [ -z "${BUILD_ONLY}" ]; then
  #           # Note [Special build images]
  #           # The xla build uses the same docker image as
  #           # pytorch_linux_bionic_py3_6_clang9_build. In the push step, we have to
  #           # distinguish between them so the test can pick up the correct image.
  #           output_image=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}
  #           if [[ ${BUILD_ENVIRONMENT} == *"xla"* ]]; then
  #             export COMMIT_DOCKER_IMAGE=$output_image-xla
  #           elif [[ ${BUILD_ENVIRONMENT} == *"libtorch"* ]]; then
  #             export COMMIT_DOCKER_IMAGE=$output_image-libtorch
  #           elif [[ ${BUILD_ENVIRONMENT} == *"paralleltbb"* ]]; then
  #             export COMMIT_DOCKER_IMAGE=$output_image-paralleltbb
  #           elif [[ ${BUILD_ENVIRONMENT} == *"parallelnative"* ]]; then
  #             export COMMIT_DOCKER_IMAGE=$output_image-parallelnative
  #           elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-x86_64"* ]]; then
  #             export COMMIT_DOCKER_IMAGE=$output_image-android-x86_64
  #           elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-arm-v7a"* ]]; then
  #             export COMMIT_DOCKER_IMAGE=$output_image-android-arm-v7a
  #           elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-arm-v8a"* ]]; then
  #             export COMMIT_DOCKER_IMAGE=$output_image-android-arm-v8a
  #           elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-x86_32"* ]]; then
  #             export COMMIT_DOCKER_IMAGE=$output_image-android-x86_32
  #           elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-vulkan-x86_32"* ]]; then
  #             export COMMIT_DOCKER_IMAGE=$output_image-android-vulkan-x86_32
  #           elif [[ ${BUILD_ENVIRONMENT} == *"vulkan-linux"* ]]; then
  #             export COMMIT_DOCKER_IMAGE=$output_image-vulkan
  #           else
  #             export COMMIT_DOCKER_IMAGE=$output_image
  #           fi
  #           docker commit "$id" ${COMMIT_DOCKER_IMAGE}
  #           time docker push ${COMMIT_DOCKER_IMAGE}
  #         fi
  #   - store_artifacts:
  #       path: /home/circleci/project/dist

  # pytorch_linux_test:
  #   <<: *pytorch_params
  #   machine:
  #     image: ubuntu-1604:202007-01
  #   steps:
  #   # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
  #   - checkout
  #   - calculate_docker_image_tag
  #   - setup_linux_system_environment
  #   - setup_ci_environment
  #   - run:
  #       name: Download Docker image
  #       no_output_timeout: "90m"
  #       command: |
  #         set -e
  #         export PYTHONUNBUFFERED=1
  #         if [[ "${DOCKER_IMAGE}" == *rocm3.9* ]]; then
  #           export DOCKER_TAG="f3d89a32912f62815e4feaeed47e564e887dffd6"
  #         fi
  #         # See Note [Special build images]
  #         output_image=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}
  #         if [[ ${BUILD_ENVIRONMENT} == *"xla"* ]]; then
  #           export COMMIT_DOCKER_IMAGE=$output_image-xla
  #         elif [[ ${BUILD_ENVIRONMENT} == *"libtorch"* ]]; then
  #           export COMMIT_DOCKER_IMAGE=$output_image-libtorch
  #         elif [[ ${BUILD_ENVIRONMENT} == *"paralleltbb"* ]]; then
  #           export COMMIT_DOCKER_IMAGE=$output_image-paralleltbb
  #         elif [[ ${BUILD_ENVIRONMENT} == *"parallelnative"* ]]; then
  #           export COMMIT_DOCKER_IMAGE=$output_image-parallelnative
  #         elif [[ ${BUILD_ENVIRONMENT} == *"vulkan-linux"* ]]; then
  #           export COMMIT_DOCKER_IMAGE=$output_image-vulkan
  #         else
  #           export COMMIT_DOCKER_IMAGE=$output_image
  #         fi
  #         echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}

  #         if [[ ${BUILD_ENVIRONMENT} == *"paralleltbb"* ]]; then
  #           echo 'ATEN_THREADING=TBB' >> "${BASH_ENV}"
  #           echo 'USE_TBB=1' >> "${BASH_ENV}"
  #         elif [[ ${BUILD_ENVIRONMENT} == *"parallelnative"* ]]; then
  #           echo 'ATEN_THREADING=NATIVE' >> "${BASH_ENV}"
  #         fi
  #         echo "Parallel backend flags: "${PARALLEL_FLAGS}

  #         time docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null

  #         # TODO: Make this less painful
  #         if [ -n "${USE_CUDA_DOCKER_RUNTIME}" ]; then
  #           export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --gpus all --shm-size=2g -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
  #         elif [[ ${BUILD_ENVIRONMENT} == *"rocm"* ]]; then
  #           hostname
  #           export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --shm-size=8g --ipc=host --device /dev/kfd --device /dev/dri --group-add video -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
  #         else
  #           export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --shm-size=1g --ipc=host -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
  #         fi
  #         echo "id=${id}" >> "${BASH_ENV}"

  #   - run:
  #       name: Check for no AVX instruction by default
  #       no_output_timeout: "20m"
  #       command: |
  #         set -e
  #         is_vanilla_build() {
  #           if [ "${BUILD_ENVIRONMENT}" == "pytorch-linux-bionic-py3.6-clang9-test" ]; then
  #             return 0
  #           fi
  #           if [ "${BUILD_ENVIRONMENT}" == "pytorch-linux-xenial-py3.6-gcc5.4-test" ]; then
  #             return 0
  #           fi
  #           return 1
  #         }

  #         if is_vanilla_build; then
  #           echo "apt-get update && apt-get install -y qemu-user gdb" | docker exec -u root -i "$id" bash
  #           echo "cd workspace/build; qemu-x86_64 -g 2345 -cpu Broadwell -E ATEN_CPU_CAPABILITY=default ./bin/basic --gtest_filter=BasicTest.BasicTestCPU & gdb ./bin/basic -ex 'set pagination off' -ex 'target remote :2345' -ex 'continue' -ex 'bt' -ex='set confirm off' -ex 'quit \$_isvoid(\$_exitcode)'" | docker exec -u jenkins -i "$id" bash
  #         else
  #           echo "Skipping for ${BUILD_ENVIRONMENT}"
  #         fi
  #   - run:
  #       name: Run tests
  #       no_output_timeout: "90m"
  #       command: |
  #         set -e

  #         cat >docker_commands.sh \<<EOL
  #         # =================== The following code will be executed inside Docker container ===================
  #         set -ex
  #         export SCRIBE_GRAPHQL_ACCESS_TOKEN="${SCRIBE_GRAPHQL_ACCESS_TOKEN}"
  #         ${PARALLEL_FLAGS}
  #         cd workspace
  #         EOL
  #         if [[ ${BUILD_ENVIRONMENT} == *"multigpu"* ]]; then
  #           echo ".jenkins/pytorch/multigpu-test.sh" >> docker_commands.sh
  #         elif [[ ${BUILD_ENVIRONMENT} == *onnx* ]]; then
  #           echo "pip install click mock tabulate networkx==2.0" >> docker_commands.sh
  #           echo "pip -q install --user \"file:///var/lib/jenkins/workspace/third_party/onnx#egg=onnx\"" >> docker_commands.sh
  #           echo ".jenkins/caffe2/test.sh" >> docker_commands.sh
  #         else
  #           echo ".jenkins/pytorch/test.sh" >> docker_commands.sh
  #         fi
  #         echo "(cat docker_commands.sh | docker exec -u jenkins -i "$id" bash) 2>&1" > command.sh
  #         unbuffer bash command.sh | ts
  #   - run:
  #       name: Report results
  #       no_output_timeout: "5m"
  #       command: |
  #         set -e
  #         docker stats --all --no-stream

  #         cat >docker_commands.sh \<<EOL
  #         # =================== The following code will be executed inside Docker container ===================
  #         set -ex
  #         export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}
  #         export SCRIBE_GRAPHQL_ACCESS_TOKEN="${SCRIBE_GRAPHQL_ACCESS_TOKEN}"
  #         export CIRCLE_TAG="${CIRCLE_TAG:-}"
  #         export CIRCLE_SHA1="$CIRCLE_SHA1"
  #         export CIRCLE_PR_NUMBER="${CIRCLE_PR_NUMBER:-}"
  #         export CIRCLE_BRANCH="$CIRCLE_BRANCH"
  #         export CIRCLE_JOB="$CIRCLE_JOB"
  #         export CIRCLE_WORKFLOW_ID="$CIRCLE_WORKFLOW_ID"
  #         cd workspace
  #         python test/print_test_stats.py --upload-to-s3 test
  #         EOL
  #         echo "(cat docker_commands.sh | docker exec -u jenkins -i "$id" bash) 2>&1" > command.sh
  #         unbuffer bash command.sh | ts

  #         echo "Retrieving test reports"
  #         docker cp $id:/var/lib/jenkins/workspace/test/test-reports ./ || echo 'No test reports found!'
  #         if [[ ${BUILD_ENVIRONMENT} == *"coverage"* ]]; then
  #             echo "Retrieving C++ coverage report"
  #             docker cp $id:/var/lib/jenkins/workspace/build/coverage.info ./test
  #         fi
  #         if [[ ${BUILD_ENVIRONMENT} == *"coverage"* || ${BUILD_ENVIRONMENT} == *"onnx"* ]]; then
  #             echo "Retrieving Python coverage report"
  #             docker cp $id:/var/lib/jenkins/workspace/test/.coverage ./test
  #             docker cp $id:/var/lib/jenkins/workspace/test/coverage.xml ./test
  #             python3 -mpip install codecov
  #             python3 -mcodecov
  #         fi
  #       when: always
  #   - store_test_results:
  #       path: test-reports

  pytorch_windows_build:
    <<: *pytorch_windows_params
    parameters:
      executor:
        type: string
        default: "windows-xlarge-cpu-with-nvidia-cuda"
      build_environment:
        type: string
        default: ""
      test_name:
        type: string
        default: ""
      cuda_version:
        type: string
        default: "10.1"
      python_version:
        type: string
        default: "3.6"
      vc_version:
        type: string
        default: "14.16"
      vc_year:
        type: string
        default: "2019"
      vc_product:
        type: string
        default: "BuildTools"
      use_cuda:
        type: string
        default: ""
    executor: <<parameters.executor>>
    steps:
      - checkout
      - run:
          name: _HACK_ Install CUDA compatible cmath
          no_output_timeout: 1m
          command: |
              powershell .circleci/scripts/vs_install_cmath.ps1
      - run:
          name: Install Cuda
          no_output_timeout: 30m
          command: |
            if [[ "${USE_CUDA}" == "1" ]]; then
              .circleci/scripts/windows_cuda_install.sh
            fi
      - run:
          name: Install Cudnn
          command : |
            if [[ "${USE_CUDA}" == "1" ]]; then
              .circleci/scripts/windows_cudnn_install.sh
            fi
      - run:
          name: Build
          no_output_timeout: "90m"
          command: |
            set -e
            set +x
            export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_WIN_BUILD_V1}
            export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_WIN_BUILD_V1}
            set -x
            .jenkins/pytorch/win-build.sh
      - persist_to_workspace:
          root: "C:/w"
          paths: build-results
      - store_artifacts:
          path: C:/w/build-results

  pytorch_windows_test:
    <<: *pytorch_windows_params
    parameters:
      executor:
        type: string
        default: "windows-medium-cpu-with-nvidia-cuda"
      build_environment:
        type: string
        default: ""
      test_name:
        type: string
        default: ""
      cuda_version:
        type: string
        default: "10.1"
      python_version:
        type: string
        default: "3.6"
      vc_version:
        type: string
        default: "14.16"
      vc_year:
        type: string
        default: "2019"
      vc_product:
        type: string
        default: "BuildTools"
      use_cuda:
        type: string
        default: ""
    executor: <<parameters.executor>>
    steps:
      - checkout
      - attach_workspace:
          at: c:/users/circleci/workspace
      - run:
          name: Install Cuda
          no_output_timeout: 30m
          command: |
            if [[ "${CUDA_VERSION}" != "cpu" ]]; then
              if [[ "${CUDA_VERSION}" != "10" || "${JOB_EXECUTOR}" != "windows-with-nvidia-gpu" ]]; then
                .circleci/scripts/windows_cuda_install.sh
              fi
            fi
      - run:
          name: Install Cudnn
          command : |
            if [[ "${CUDA_VERSION}" != "cpu" ]]; then
              .circleci/scripts/windows_cudnn_install.sh
            fi
      - run:
          name: Test
          no_output_timeout: "30m"
          command: |
            set -e
            export IN_CI=1
            set +x
            export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_WIN_BUILD_V1}
            export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_WIN_BUILD_V1}
            set -x
            .jenkins/pytorch/win-test.sh
      - store_test_results:
          path: test/test-reports
  # binary_linux_build:
  #   <<: *binary_linux_build_params
  #   steps:
  #   - checkout
  #   - calculate_docker_image_tag
  #   - run:
  #       <<: *binary_checkout
  #   - run:
  #       <<: *binary_populate_env
  #   - run:
  #       name: Build
  #       no_output_timeout: "1h"
  #       command: |
  #           source "/pytorch/.circleci/scripts/binary_linux_build.sh"
  #           # Preserve build log
  #           if [ -f /pytorch/build/.ninja_log ]; then
  #             cp /pytorch/build/.ninja_log /final_pkgs
  #           fi
  #   - run:
  #       name: Output binary sizes
  #       no_output_timeout: "1m"
  #       command: |
  #           ls -lah /final_pkgs
  #   - run:
  #       name: save binary size
  #       no_output_timeout: "5m"
  #       command: |
  #           source /env
  #           cd /pytorch && export COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
  #           python3 -mpip install requests && \
  #           SCRIBE_GRAPHQL_ACCESS_TOKEN=${SCRIBE_GRAPHQL_ACCESS_TOKEN} \
  #           python3 /pytorch/.circleci/scripts/upload_binary_size_to_scuba.py || exit 0
  #   - persist_to_workspace:
  #       root: /
  #       paths: final_pkgs

  #   - store_artifacts:
  #       path: /final_pkgs

  #   # This should really just be another step of the binary_linux_build job above.
  #   # This isn't possible right now b/c the build job uses the docker executor
  #   # (otherwise they'd be really really slow) but this one uses the macine
  #   # executor (b/c we have to run the docker with --runtime=nvidia and we can't do
  #   # that on the docker executor)
  # binary_linux_test:
  #   <<: *binary_linux_test_upload_params
  #   machine:
  #       image: ubuntu-1604:202007-01
  #   steps:
  #   # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
  #   - checkout
  #   - attach_workspace:
  #       at: /home/circleci/project
  #   - setup_linux_system_environment
  #   - setup_ci_environment
  #   - run:
  #       <<: *binary_checkout
  #   - run:
  #       <<: *binary_populate_env
  #   - run:
  #       name: Prepare test code
  #       no_output_timeout: "1h"
  #       command: .circleci/scripts/binary_linux_test.sh
  #   - run:
  #       <<: *binary_run_in_docker

  binary_upload:
    parameters:
      package_type:
        type: string
        description: "What type of package we are uploading (eg. wheel, libtorch, conda)"
        default: "wheel"
      upload_subfolder:
        type: string
        description: "What subfolder to put our package into (eg. cpu, cudaX.Y, etc.)"
        default: "cpu"
    docker:
      - image: continuumio/miniconda3
    environment:
      - DRY_RUN: disabled
      - PACKAGE_TYPE: "<< parameters.package_type >>"
      - UPLOAD_SUBFOLDER: "<< parameters.upload_subfolder >>"
    steps:
      - attach_workspace:
          at: /tmp/workspace
      - checkout
      - designate_upload_channel
      - run:
          name: Install dependencies
          no_output_timeout: "1h"
          command: |
            conda install -yq anaconda-client
            pip install -q awscli
      - run:
          name: Do upload
          no_output_timeout: "1h"
          command: |
            AWS_ACCESS_KEY_ID="${PYTORCH_BINARY_AWS_ACCESS_KEY_ID}" \
              AWS_SECRET_ACCESS_KEY="${PYTORCH_BINARY_AWS_SECRET_ACCESS_KEY}" \
              ANACONDA_API_TOKEN="${CONDA_PYTORCHBOT_TOKEN}" \
              .circleci/scripts/binary_upload.sh

  # Nighlty build smoke tests defaults
  # These are the second-round smoke tests. These make sure that the binaries are
  # correct from a user perspective, testing that they exist from the cloud are
  # are runnable. Note that the pytorch repo is never cloned into these jobs
  ##############################################################################
  # smoke_linux_test:
  #   <<: *binary_linux_test_upload_params
  #   machine:
  #     image: ubuntu-1604:202007-01
  #   steps:
  #   - checkout
  #   - calculate_docker_image_tag
  #   - setup_linux_system_environment
  #   - setup_ci_environment
  #   - run:
  #       <<: *binary_checkout
  #   - run:
  #       <<: *binary_populate_env
  #   - run:
  #       name: Test
  #       no_output_timeout: "1h"
  #       command: |
  #         set -ex
  #         cat >/home/circleci/project/ci_test_script.sh \<<EOL
  #         # The following code will be executed inside Docker container
  #         set -eux -o pipefail
  #         /builder/smoke_test.sh
  #         # The above code will be executed inside Docker container
  #         EOL
  #   - run:
  #       <<: *binary_run_in_docker

  # smoke_mac_test:
  #   <<: *binary_linux_test_upload_params
  #   macos:
  #     xcode: "12.0"
  #   steps:
  #     - checkout
  #     - run:
  #         <<: *binary_checkout
  #     - run:
  #         <<: *binary_populate_env
  #     - brew_update
  #     - run:
  #         <<: *binary_install_miniconda
  #     - run:
  #         name: Build
  #         no_output_timeout: "1h"
  #         command: |
  #           set -ex
  #           source "/Users/distiller/project/env"
  #           export "PATH=$workdir/miniconda/bin:$PATH"
  #           # TODO unbuffer and ts this, but it breaks cause miniconda overwrites
  #           # tclsh. But unbuffer and ts aren't that important so they're just
  #           # disabled for now
  #           ./builder/smoke_test.sh

  # binary_mac_build:
  #   <<: *binary_mac_params
  #   macos:
  #     xcode: "12.0"
  #   steps:
  #   # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
  #   - checkout
  #   - run:
  #       <<: *binary_checkout
  #   - run:
  #       <<: *binary_populate_env
  #   - brew_update
  #   - run:
  #       <<: *binary_install_miniconda

  #   - run:
  #       name: Build
  #       no_output_timeout: "90m"
  #       command: |
  #         # Do not set -u here; there is some problem with CircleCI
  #         # variable expansion with PROMPT_COMMAND
  #         set -ex -o pipefail
  #         script="/Users/distiller/project/pytorch/.circleci/scripts/binary_macos_build.sh"
  #         cat "$script"
  #         source "$script"

  #   - run:
  #       name: Test
  #       no_output_timeout: "1h"
  #       command: |
  #         # Do not set -u here; there is some problem with CircleCI
  #         # variable expansion with PROMPT_COMMAND
  #         set -ex -o pipefail
  #         script="/Users/distiller/project/pytorch/.circleci/scripts/binary_macos_test.sh"
  #         cat "$script"
  #         source "$script"

  #   - persist_to_workspace:
  #       root: /Users/distiller/project
  #       paths: final_pkgs

  #   - store_artifacts:
  #       path: /Users/distiller/project/final_pkgs

  # binary_macos_arm64_build:
  #   <<: *binary_mac_params
  #   macos:
  #     xcode: "12.3.0"
  #   steps:
  #   # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
  #   - checkout
  #   - run:
  #       <<: *binary_checkout
  #   - run:
  #       <<: *binary_populate_env
  #   - brew_update
  #   - run:
  #       <<: *binary_install_miniconda

  #   - run:
  #       name: Build
  #       no_output_timeout: "90m"
  #       command: |
  #         # Do not set -u here; there is some problem with CircleCI
  #         # variable expansion with PROMPT_COMMAND
  #         set -ex -o pipefail
  #         export CROSS_COMPILE_ARM64=1
  #         script="/Users/distiller/project/pytorch/.circleci/scripts/binary_macos_build.sh"
  #         cat "$script"
  #         source "$script"

  #   - persist_to_workspace:
  #       root: /Users/distiller/project
  #       paths: final_pkgs

  #   - store_artifacts:
  #       path: /Users/distiller/project/final_pkgs


  # binary_ios_build:
  #   <<: *pytorch_ios_params
  #   macos:
  #     xcode: "12.0"
  #   steps:
  #   - attach_workspace:
  #       at: ~/workspace
  #   - checkout
  #   - run_brew_for_ios_build
  #   - run:
  #       name: Build
  #       no_output_timeout: "1h"
  #       command: |
  #         script="/Users/distiller/project/.circleci/scripts/binary_ios_build.sh"
  #         cat "$script"
  #         source "$script"
  #   - run:
  #       name: Test
  #       no_output_timeout: "30m"
  #       command: |
  #         script="/Users/distiller/project/.circleci/scripts/binary_ios_test.sh"
  #         cat "$script"
  #         source "$script"
  #   - persist_to_workspace:
  #       root: /Users/distiller/workspace/
  #       paths: ios

  # binary_ios_upload:
  #   <<: *pytorch_ios_params
  #   macos:
  #     xcode: "12.0"
  #   steps:
  #   - attach_workspace:
  #       at: ~/workspace
  #   - checkout
  #   - run_brew_for_ios_build
  #   - run:
  #       name: Upload
  #       no_output_timeout: "1h"
  #       command: |
  #         script="/Users/distiller/project/.circleci/scripts/binary_ios_upload.sh"
  #         cat "$script"
  #         source "$script"

  binary_windows_build:
    <<: *binary_windows_params
    parameters:
      build_environment:
        type: string
        default: ""
      executor:
        type: string
        default: "windows-xlarge-cpu-with-nvidia-cuda"
    executor: <<parameters.executor>>
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - checkout
    - run:
        name: _HACK_ Install CUDA compatible cmath
        no_output_timeout: 1m
        command: |
            powershell .circleci/scripts/vs_install_cmath.ps1
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        name: Build
        no_output_timeout: "1h"
        command: |
          set -eux -o pipefail
          script="/c/w/p/.circleci/scripts/binary_windows_build.sh"
          cat "$script"
          source "$script"
    - persist_to_workspace:
        root: "C:/w"
        paths: final_pkgs

  binary_windows_test:
    <<: *binary_windows_params
    parameters:
      build_environment:
        type: string
        default: ""
      executor:
        type: string
        default: "windows-medium-cpu-with-nvidia-cuda"
    executor: <<parameters.executor>>
    steps:
    - checkout
    - attach_workspace:
        at: c:/users/circleci/project
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        name: Test
        no_output_timeout: "1h"
        command: |
          set -eux -o pipefail
          script="/c/w/p/.circleci/scripts/binary_windows_test.sh"
          cat "$script"
          source "$script"

  smoke_windows_test:
    <<: *binary_windows_params
    parameters:
      build_environment:
        type: string
        default: ""
      executor:
        type: string
        default: "windows-medium-cpu-with-nvidia-cuda"
    executor: <<parameters.executor>>
    steps:
    - checkout
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        name: Test
        no_output_timeout: "1h"
        command: |
          set -eux -o pipefail
          export TEST_NIGHTLY_PACKAGE=1
          script="/c/w/p/.circleci/scripts/binary_windows_test.sh"
          cat "$script"
          source "$script"

  # anaconda_prune:
  #   parameters:
  #     packages:
  #       type: string
  #       description: "What packages are we pruning? (quoted, space-separated string. eg. 'pytorch', 'torchvision torchaudio', etc.)"
  #       default: "pytorch"
  #     channel:
  #       type: string
  #       description: "What channel are we pruning? (eq. pytorch-nightly)"
  #       default: "pytorch-nightly"
  #   docker:
  #     - image: continuumio/miniconda3
  #   environment:
  #     - PACKAGES: "<< parameters.packages >>"
  #     - CHANNEL: "<< parameters.channel >>"
  #   steps:
  #     - checkout
  #     - run:
  #         name: Install dependencies
  #         no_output_timeout: "1h"
  #         command: |
  #           conda install -yq anaconda-client
  #     - run:
  #         name: Prune packages
  #         no_output_timeout: "1h"
  #         command: |
  #             ANACONDA_API_TOKEN="${CONDA_PYTORCHBOT_TOKEN}" \
  #             scripts/release/anaconda-prune/run.sh

##############################################################################
# Workflows
##############################################################################
workflows:
  binary_builds:
    jobs:
      - binary_windows_build:
          name: binary_windows_wheel_3_7_cu111_nightly_build
          build_environment: "wheel 3.7 cu111"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
      - binary_windows_build:
          name: binary_windows_libtorch_3_7_cu111_release_nightly_build
          build_environment: "libtorch 3.7 cu111 release"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
    when: << pipeline.parameters.run_binary_tests >>
  build:
    jobs:
      - pytorch_windows_build:
          build_environment: pytorch-win-vs2019-cuda11-cudnn8-py3
          cuda_version: "11.1"
          name: pytorch_windows_vs2019_py36_cuda11.1_build
          python_version: "3.6"
          use_cuda: "1"
          vc_product: Community
          vc_version: ""
          vc_year: "2019"
      - pytorch_windows_test:
          build_environment: pytorch-win-vs2019-cuda11-cudnn8-py3
          cuda_version: "11.1"
          executor: windows-with-nvidia-gpu
          filters:
            branches:
              only:
                - master
                - /ci-all\/.*/
                - /release\/.*/
          name: pytorch_windows_vs2019_py36_cuda11.1_test1
          python_version: "3.6"
          requires:
            - pytorch_windows_vs2019_py36_cuda11.1_build
          test_name: pytorch-windows-test1
          use_cuda: "1"
          vc_product: Community
          vc_version: ""
          vc_year: "2019"
      - pytorch_windows_test:
          build_environment: pytorch-win-vs2019-cuda11-cudnn8-py3
          cuda_version: "11.1"
          executor: windows-with-nvidia-gpu
          filters:
            branches:
              only:
                - master
                - /ci-all\/.*/
                - /release\/.*/
          name: pytorch_windows_vs2019_py36_cuda11.1_test2
          python_version: "3.6"
          requires:
            - pytorch_windows_vs2019_py36_cuda11.1_build
          test_name: pytorch-windows-test2
          use_cuda: "1"
          vc_product: Community
          vc_version: ""
          vc_year: "2019"
      # - update_s3_htmls:
      #     context: org-member
      #     filters:
      #       branches:
      #         only:
      #           - postnightly
      #     name: update_s3_htmls
    when: << pipeline.parameters.run_build >>
